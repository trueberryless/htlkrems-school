Was sind große Sprachmodelle?
Groß Sprachmodelle (LLM) sind sehr große Deep-Learning-Modelle, die auf riesigen Datenmengen vorab trainiert wurden. Der zugrundeliegende Transformator ist eine Reihe von neuronalen Netzwerken, die aus einem Encoder und einem Decoder mit Selbstbeobachtungsfunktion bestehen. Der Encoder und der Decoder extrahieren Bedeutungen aus einer Textsequenz und verstehen die Beziehungen zwischen Wörtern und Phrasen darin.

Transformer-LLMs sind in der Lage, unbeaufsichtigt zu trainieren, obwohl eine genauere Erklärung darin besteht, dass Transformatoren selbstlernend sind. Durch diesen Prozess lernen Transformatoren, grundlegende Grammatik, Sprachen und Kenntnisse zu verstehen.

Im Gegensatz zu früheren rekurrenten neuronalen Netzwerken (RNN), die Eingaben sequentiell verarbeiten, verarbeiten Transformatoren ganze Sequenzen parallel. Dies ermöglicht es den Datenwissenschaftlern, GPUs für das Training transformatorbasierter LLMs zu verwenden, wodurch die Trainingszeit erheblich reduziert wird.

Die neuronale Netzwerkarchitektur von Transformatoren ermöglicht die Verwendung sehr großer Modelle, oft mit Hunderten von Milliarden von Parametern. Solche groß angelegten Modelle können riesige Datenmengen aufnehmen, oft aus dem Internet, aber auch aus Quellen wie dem Common Crawl, der mehr als 50 Milliarden Webseiten umfasst, und Wikipedia mit etwa 57 Millionen Seiten.

Weitere Informationen über neuronale Netzwerke »

Lesen Sie mehr über Deep Learning »

Warum sind große Sprachmodelle wichtig?
Große Sprachmodelle sind unglaublich flexibel. Ein Modell kann völlig unterschiedliche Aufgaben ausführen, z. B. Fragen beantworten, Dokumente zusammenfassen, Sprachen übersetzen und Sätze vervollständigen. LLMs haben das Potenzial, die Erstellung von Inhalten und die Art und Weise, wie Menschen Suchmaschinen und virtuelle Assistenten nutzen, zu stören.

LLMs sind zwar nicht perfekt, zeigen aber eine bemerkenswerte Fähigkeit, Vorhersagen auf der Grundlage einer relativ geringen Anzahl von Eingabeaufforderungen oder Eingaben zu treffen. LLMs können für generative KI (künstliche Intelligenz) verwendet werden, um Inhalte auf der Grundlage von Eingabeaufforderungen in menschlicher Sprache zu erstellen.

LLMs sind groß, sehr groß. Sie können Milliarden von Parametern berücksichtigen und haben viele Einsatzmöglichkeiten. Hier sind einige Beispiele:

Das GPT-3-Modell von Open AI hat 175 Milliarden Parameter. Sein Cousin, ChatGPT, kann Muster aus Daten identifizieren und natürliche und lesbare Ergebnisse erzeugen. Wir wissen zwar nicht, wie groß Claude 2 ist, aber es kann bis zu 100.000 Tokens pro Eingabeaufforderung eingeben, was bedeutet, dass es Hunderte von Seiten technischer Dokumentation oder sogar ein ganzes Buch verarbeiten kann.
Das Jurassic-1-Modell von AI21 Labs verfügt über 178 Milliarden Parameter und ein symbolisches Vokabular mit 250.000 Wortteilen und ähnlichen Konversationsmöglichkeiten.
Das Command-Modell von Cohere verfügt über ähnliche Funktionen und kann in mehr als 100 verschiedenen Sprachen verwendet werden.
Paradigm von LightON bietet Basismodelle mit behaupteten Fähigkeiten, die die von GPT-3 übertreffen. Alle diese LLMs verfügen über APIs, mit denen Entwickler einzigartige generative KI-Anwendungen erstellen können.
Lesen Sie mehr über generative KI »

Lesen Sie mehr über Basismodelle »

Wie funktionieren große Sprachmodelle?
Ein Schlüsselfaktor für die Funktionsweise von LLMs ist die Art und Weise, wie sie Wörter darstellen. Frühere Formen des Machine Learning verwendeten eine numerische Tabelle, um jedes Wort darzustellen. Diese Darstellungsform konnte jedoch keine Beziehungen zwischen Wörtern erkennen, beispielsweise Wörter mit ähnlicher Bedeutung. Diese Einschränkung wurde überwunden, indem mehrdimensionale Vektoren, die allgemein als Worteinbettungen bezeichnet werden, verwendet wurden, um Wörter so darzustellen, dass Wörter mit ähnlichen kontextuellen Bedeutungen oder anderen Beziehungen im Vektorraum nahe beieinander liegen.

Mithilfe von Worteinbettungen können Transformatoren Text als numerische Repräsentationen durch den Encoder vorverarbeiten und so den Kontext von Wörtern und Phrasen mit ähnlicher Bedeutung sowie andere Beziehungen zwischen Wörtern, z. B. Wortarten, verstehen. Es ist dann für LLMs möglich, dieses Sprachwissen über den Decoder anzuwenden, um eine einzigartige Ausgabe zu erzeugen.

Was sind Anwendungen großer Sprachmodelle?
Es gibt viele praktische Anwendungen für LLMs.

Copywriting
Neben GPT-3 und ChatGPT schreiben Claude, Llama 2, Cohere Command und Jurassiccan die Originalkopie. AI21 Wordspice schlägt Änderungen an den Originalsätzen vor, um Stil und Stimme zu verbessern.

Beantwortung in der Wissensdatenbank
Die Technik wird oft als wissensintensive Verarbeitung natürlicher Sprache (KI-NLP) bezeichnet und bezieht sich auf LLMs, die spezifische Fragen aus der Informationshilfe in digitalen Archiven beantworten können. Ein Beispiel ist die Fähigkeit von AI21 Studio Playground, Fragen des Allgemeinwissens zu beantworten.

Textklassifizierung
Mithilfe von Clustering können LLMs Text mit ähnlichen Bedeutungen oder Stimmungen klassifizieren. Zu den Anwendungen gehören die Messung der Kundenstimmung, die Bestimmung der Beziehung zwischen Texten und die Suche nach Dokumenten.

Code-Generierung
LLM beherrschen die Codegenerierung aus Eingabeaufforderungen in natürlicher Sprache. Beispiele hierfür sind Amazon CodeWhisperer und der in GitHub Copilot verwendete Codex von Open AI, der in Python, JavaScript, Ruby und mehreren anderen Programmiersprachen codieren kann. Andere Codierungsanwendungen umfassen das Erstellen von SQL-Abfragen, das Schreiben von Shell-Befehlen und das Design von Websites.

Textgenerierung
Ähnlich wie bei der Codegenerierung können mit der Textgenerierung unvollständige Sätze vervollständigt, Produktdokumentationen geschrieben oder, wie Alexa Create, eine kurze Kindergeschichte geschrieben werden.

Wie werden große Sprachmodelle geschult?
Transformatorbasierte neuronale Netze sind sehr groß. Diese Netzwerke enthalten mehrere Knoten und Schichten. Jeder Knoten in einer Schicht hat Verbindungen zu allen Knoten in der nachfolgenden Schicht, von denen jeder eine Gewichtung und eine Verzerrung hat. Gewichte und Verzerrungen werden zusammen mit Einbettungen als Modellparameter bezeichnet. Große transformatorbasierte neuronale Netze können Milliarden und Abermilliarden von Parametern haben. Die Größe des Modells wird im Allgemeinen durch eine empirische Beziehung zwischen der Modellgröße, der Anzahl der Parameter und der Größe der Trainingsdaten bestimmt.

Die Schulung wird mit einem großen Korpus hochwertiger Daten durchgeführt. Während der Schulung passt das Modell die Parameterwerte iterativ an, bis das Modell das nächste Token aus der vorherigen Sequenz von Eingabe-Token korrekt vorhersagt. Dies geschieht durch selbstlernende Techniken, die dem Modell beibringen, Parameter anzupassen, um die Wahrscheinlichkeit zu maximieren, dass die nächsten Token in den Schulungsbeispielen erscheinen.

Einmal geschult, können LLMs leicht angepasst werden, um mehrere Aufgaben mit relativ kleinen Mengen überwachter Daten auszuführen, ein Prozess, der als Feinabstimmung bezeichnet wird.

Es gibt drei gängige Lernmodelle:

Zero-Shot-Learning: Base-LLMs können ohne explizite Schulung auf eine Vielzahl von Anfragen reagieren, oft durch Eingabeaufforderungen, obwohl die Antwortgenauigkeit unterschiedlich ist.
Lernen mit wenigen Klicks: Durch die Bereitstellung einiger relevanter Schulungsbeispiele verbessert sich die Leistung des Basismodells in diesem speziellen Bereich erheblich.
Feinabstimmung: Dabei handelt es sich um eine Erweiterung des Few-Shot Learning, bei der Datenwissenschaftler ein Basismodell so trainieren, dass es seine Parameter mit zusätzlichen Daten, die für die spezifische Anwendung relevant sind, anpasst.
Wie sieht die Zukunft der LLMs aus?
Die Einführung großer Sprachmodelle wie ChatGPT, Claude 2 und Llama 2, die Fragen beantworten und Text generieren können, weist auf spannende Möglichkeiten in der Zukunft hin. Langsam, aber sicher nähern sich LLMs einer menschenähnlichen Leistung. Der unmittelbare Erfolg dieser LLMs zeigt ein großes Interesse an roboterartigen LLMs, die das menschliche Gehirn emulieren und in einigen Zusammenhängen übertreffen. Hier sind einige Gedanken zur Zukunft von LLMs,

Erhöhte Fähigkeiten
So beeindruckend sie auch sind, der aktuelle Stand der Technologie ist nicht perfekt und LLMs sind nicht unfehlbar. Neuere Versionen werden jedoch über eine verbesserte Genauigkeit und erweiterte Funktionen verfügen, da Entwickler lernen, ihre Leistung zu verbessern und gleichzeitig Verzerrungen zu reduzieren und falsche Antworten zu vermeiden.

Audiovisuelle Schulung
Während Entwickler die meisten LLMs mit Text trainieren, haben einige damit begonnen, Modelle mit Video- und Audioeingängen zu trainieren. Diese Form der Ausbildung sollte zu einer schnelleren Modellentwicklung führen und neue Möglichkeiten im Hinblick auf den Einsatz von LLMs für autonome Fahrzeuge eröffnen.

Transformation des Arbeitsplatzes
LLMs sind ein Störfaktor, der den Arbeitsplatz verändern wird. LLMs werden wahrscheinlich monotone und sich wiederholende Aufgaben auf die gleiche Weise reduzieren wie Roboter bei sich wiederholenden Fertigungsaufgaben. Zu den Möglichkeiten gehören sich wiederholende Büroaufgaben, Kundenservice-Chatbots und einfaches automatisiertes Copywriting.

Dialogorientierte KI
LLMs werden zweifellos die Leistung automatisierter virtueller Assistenten wie Alexa, Google Assistant und Siri verbessern. Sie werden besser in der Lage sein, Benutzerabsichten zu interpretieren und auf ausgeklügelte Befehle zu reagieren.

Wie kann AWS bei LLMs helfen?
AWS bietet Entwicklern großer Sprachmodelle mehrere Möglichkeiten. Amazon Bedrock ist die einfachste Möglichkeit, generative KI-Anwendungen mit Basismodellen (FM) zu erstellen und zu skalieren. Amazon Bedrock ist ein vollständig verwalteter Service, der LLMs von Amazon und führenden KI-Startups über eine API zur Verfügung stellt. So können Sie aus verschiedenen FMs das Modell auswählen, das für Ihren Anwendungsfall am besten geeignet ist.

Amazon SageMaker JumpStart ist ein Hub für Machine Learning mit Basismodellen, integrierten Algorithmen und vorgefertigten ML-Lösungen, die Sie mit nur wenigen Klicks bereitstellen können. Mit SageMaker JumpStart können Sie auf vortrainierte Modelle, einschließlich Basismodelle, zugreifen, um Aufgaben wie Artikelzusammenfassung und Bildgenerierung auszuführen. Vortrainierte Modelle sind mit Ihren Daten vollständig an Ihren Anwendungsfall anpassbar, und Sie können sie mit der Benutzeroberfläche oder dem SDK problemlos in der Produktion einsetzen.